\documentclass{article}

\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry}
\usepackage{dsfont}

% ===== This makes my \affil cmnd work.
\usepackage[affil-it]{authblk}


% ===== This makes my environments work switching llncs to article.
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi}

% ===== For \algorithm. Is this a decent idea?
\usepackage[lined,boxed,ruled,vlined]{algorithm2e}

% ===== For \mathscr
\usepackage{mathrsfs}
\DeclareSymbolFontAlphabet{\mathrsfs}{rsfs}
\usepackage[mathscr]{eucal}


% ===== For \boldsymbol
\usepackage{amsbsy}

% ===== For \bm (bold math)
\usepackage{bm}

\usepackage{fixltx2e}
\MakeRobust{\overrightarrow}

% ==== Misha and Ning's Notation file =====
\include{macrofile2a}

%%%% Dr. K's colored comments. 
\usepackage{color} 
\definecolor{blue}{rgb}{0,0,1}
\definecolor{red}{rgb}{1,0,0}
\definecolor{purple}{rgb}{1,0,1}
\newcommand\MEK[1]{\textcolor{red}{MEK: #1}}
\newcommand\EMK[1]{\textcolor{purple}{EMK: #1}}
\newcommand\SA[1]{\textcolor{blue}{SA: #1}}

\begin{document}


\title{Basis Expansion Monte Carlo}

\author{Eric Kernfeld
  \thanks{Electronic address: \texttt{ekernf01@u.washington.edu}; Corresponding author}}
\affil{University of Washington, Seattle, WA, USA}
\maketitle

\begin{abstract}
We introduce Basis Expansion Monte Carlo, which studies a Gibbs or Metropolis-Hastings sampler to infer the underlying transition kernel. To make inference about the steady state, which is usually the item of interest in a sampler, we compute the steady-state of the approximate kernel. Results show ...
\end{abstract}


\section{Introduction}
In many statistical models, it is impossible to find a closed form for the distribution of interest (we will call this $\pi$). One work-around, originating in computational physics, relies on the fact that for points $x_1$ and $x_2$ in the parameter space, $\pi(x_1)/\pi(x_2)$ may still be calculable, though $\pi(x_1)$ and $\pi(x_2)$ are not. This fact is exploited to produce a Markov chain whose steady-state distribution is guaranteed to be $\pi$. 

More and references about history, background, and/or tutorials on monte carlo methods



One popular method, the Metropolis-Hastings scheme, consists of the following procedure.

\begin{algorithm}[h]
\caption{Metropolis-Hastings algorithm}
Set $x_0 = 0, i=0$\\
Repeat ad nauseum:\\
\Indp
Increment $i$\\
Draw $x$ from a proposal distribution $q(x|x_{i-1})$\\
Set $\alpha(x_{}|x_{i-1}) = \min(1, \frac{\pi(x)q(x_{i-1}|x)}{\pi(x_{i-1})q(x|x_{i-1})})$\\
Set $x_i = x$ with probability $\alpha$ and $x_i = x_{i-1}$ as the complementary event.\\
\end{algorithm}

Suppose this MCMC algorithm produces a chain $ x_1, x_2, x_3, ...$ of samples. Because the algorithm is stochastic, these samples can be viewed as realizations of random variables $X_1, X_2, X_3, ...$ with marginal density functions $f_1, f_2, f_3, $ etc. Because $X_i$ is independent of past draws given $X_{i-1}$, the conditional distribution $f_{i|i-1}(x_i|x_{i-1})$ contains all the information we need about this method. In particular, to move forward an iteration, we can write $f_i(x_i) = \int f_{i|i-1}(x_{i},x_{i-1})f_{i-1}(x_{i-1})dx_{i-1}$. Noting that $f_{i|i-1}$ doesn't depend on $i$, we can replace it with a function $K$ so that $f_i(x_i) = \int K(x_i, x_{i-1})f_{i-1}(x_{i-1})dx_{i-1}$. This function $K$, called the Markov kernel, is analogous to the transition probability matrices of discrete-space Markov chain theory. We refer to the linear operator of integrating against $K$ as $L$, so that $f_{i} = Lf_{i-1}$. The object of interest is the steady state of this operator, an eigenfunction $\pi$ that has eigenvalue $1$ so that for any $x$, $\pi(x) = (L\pi)(x) = \int K(x, t)\pi(t)dt$. 

In MCMC methods, chains are usually left to run until the Markov chain converges to its stationary distribution. By contrast, in BEMC, we approximate $L$ by methods that allow the chain to run briefly from many different places, and we compute $\pi$ from the approximation. 

\subsection{Stage one: approximating the kernel}
\label{sec:BEMC}

\subsection{Notation and setup}

Our estimator begins with a fixed set of basis functions $\{h_i\}_1^B$. We will have two sources of error: discretization from the finite basis and stochasticity from the sample. Object suffering from discretization error will be labeled with tildes, while objects also suffering from stochasticity will be labeled with hats. There is one exception: we estimate a matrix called $M$, with both error sources present, and it will appear with no hat.

For an unknown transition kernel $K$, define $\tilde{M}$ to be the matrix that minimizes the squared $\mathcal{L}_2$ distance $\|K - \sum_{i,j}^B\tilde{m}_{ij} h_i \otimes h_j\|^2$, or in other terms the matrix that minimizes the integral $$\int_{\Omega\times\Omega}(K(x,y) - \sum_{i,j}^B\tilde{m}_{ij} h_i(x)h_j(y))^2dx dy .$$ \EMK{Surely this minimum must exist for a quadratic?} Then define $\tilde{K}(x,y) \equiv \sum_{i,j=1}^B \tilde{m}_{ij} h_i(x)h_j(y)$ and define $\tilde{L}$ so that $(\tilde{L}f)(x)$ is $$\int_{\Omega}\tilde{K}(x,y)f(y)dy.$$

We will attempt to estimate $\tilde{M}$ using a matrix $M$, and the corresponding approximation for $K$ will be 
\begin{equation}
\label{eqn:khatdef}
\hat{K}(x,y) = \sum_{i,j=1}^B M_{ij} h_i(x)h_j(y).\end{equation}

Similar to before, $\hat{L}$ is defined so that $(\hat{L}f)(x)$ is $$\int_{\Omega}\hat{K}(x,y)f(y)dy.$$


We define a matrix $\tilde{G}$ elementwise so that $\tilde{g}_{ij}\equiv \int_{\Omega} h_i(x)(\tilde{L}h_j)(x)dx$, with the corresponding statement for hatted variables so that $\hat{g}_{ij}\equiv \int_{\Omega} h_i(x)(\hat{L}h_j)(x)dx$. We also make use of the $\mathcal{L}_2$ inner products $c_{ij}\equiv \int_{\Omega} h_i(x)h_j(x)dx$.

\subsection{The estimator}

Since $\hat{g}_{ij}$ expands as $$ \int_{\Omega} h_i(x)\left[\sum_{\ell,k} M_{k\ell}h_k(x)\int_{\Omega} h_\ell(y)h_j(y)dy\right]dx= \sum_{\ell,k} M_{k\ell}C_{\ell j}C_{ik},$$ we know that $\hat{G}=CMC$. So, if we can estimate $G$, we can estimate $M$ as $C^{-1}GC^{-1}$. We assume $C$ is readily calculable and not too badly conditioned, and we now focus on $G$. One nice special case of this formula: for some choices of $\{h_i\}_1^B$, $C$ is the identity matrix and $\hat{G}=M$. 

Think of $Lh_j$ as a probability distribution: it corresponds to initializing the sample from a draw $z\sim h_j$, then running a single step of Metropolis-Hastings. By definition, $G$ can be written as an expectation $G_{ij} = E_{Lh_j}[h_i]$. This motivates us to sample from a normalized version of $Lh_j$ and approximate $G_{ij}$ as an average. All we need to do is sample $z$ from $h_j$, run an M-H iteration on $z$ to get $w$, and retain $w$ as our sample from $Lh_j$. \EMK{Conjecture: the hats converge to the tildes as you run it for longer, and the tildes converge to the truth as you lengthen the basis.}

\subsubsection{Basic Estimator Properties}
Our approximation can only imitate continuous kernels, i.e. situations where $\int K(x,y)f(y)dy$ can be done with respect to the Lebesgue measure. This presents an obstacle, because with positive probability, the Metropolis-Hastings algorithm will reject a proposed sample and stay in place. As a workaround, we can approximate the kernel not of a single M-H iteration but of $r$ iterations for $r$ around 10 or 20. The probability of $r$ consecutive rejections is much smaller, pushing the true kernel closer to the subspace in which we approximate it. In section \ref{sec:BEMC-R}, we discuss a variant that explicitly models rejection events.

\subsubsection{A more concrete procedure using a Gaussian basis}
To start out, we will use multivariate Gaussian densities as our basis so that when $\Omega=\mathbb{R}^D$, $h_i(x)= (\frac{1}{2\pi \sigma_i^2})^{\frac{D}{2}}\exp(\frac{(x-\mu_i)^2}{\sigma_i^2})$. Then, $C_{ij}$ can be computed as follows.
\begin{align*}
\int h_i(x)h_j(x)dx &=\int (\frac{1}{2\pi \sigma_i\sigma_j})^{D}\exp(\frac{-(x-\mu_i)^2}{2\sigma_i^2}+\frac{-(x-\mu_j)^2}{2\sigma_j^2})dx\\
&=\int_{x_1=x_2} (\frac{1}{2\pi \sigma_i\sigma_j})^{D}\exp(-\frac{(x_1-\mu_i)^2}{2\sigma_i^2}+\frac{-(x_2-\mu_j)^2}{2\sigma_j^2})dx_1dx_2\\
&=\int_{y_1-y_2+\mu_i-\mu_j=0} (\frac{1}{2\pi \sigma_i\sigma_j})^{D}\exp(\frac{-y_1^2}{2\sigma_i^2}+\frac{-y_2^2}{2\sigma_j^2})dy_1dy_2\\
&=\int_{\sigma_iz_1-\sigma_jz_2+\mu_i-\mu_j=0}
 (\frac{1}{2\pi \sigma_i\sigma_j})^{D}
\exp(\frac{-z_1^2-z_2^2}{2})(\sigma_i\sigma_j)^D dz_1dz_2\\
&=\int_{\sigma_iz_1-\sigma_jz_2+\mu_i-\mu_j=0}
 (\frac{1}{2\pi })^{D}
\exp(\frac{-z_1^2-z_2^2}{2}) dz_1dz_2\\
&=f_u(0) \text{ if } u=\sigma_iz_1-\sigma_jz_2+\mu_i-\mu_j \text{ and $z$'s are standard normal}\\
&= (\frac{1}{2\pi (\sigma_i+\sigma_j)^2})^{\frac{D}{2}}\exp(\frac{-(\mu_i-\mu_j)^2}{2\sigma_i^2+2\sigma_j^2})
\end{align*}

\begin{algorithm}[h]
\caption{BEMC algorithm--stage one}
Set $M$ to a matrix of all zeroes.\\
For $i  = 1:B$\\
\Indp
For $j  = 1:B$\\
\Indp
For $n = 1:N$\\
\Indp
Draw a sample $z_n$ from $h_{j}$, i.e. a normal draw with mean $\mu_j$ and variance $\sigma_j^2$.\\
Run the M-H sampler for $\ell$ rounds on $z_n$. Call the result $w_n$.\\
Increment $G_{i, j}$ by $h_{i}(w_n)/N$.\\
\Indm
\Indm
\Indm

\end{algorithm}

\section{Stage two: calculating the steady state}

We now explain how to get, from our approximation for the kernel, an approximation for the target. We make use of the power method, a simple algorithm for eigenvector computation. Given a matrix $M$, the power method computes $v\leftarrow M^v$ for any initial vector $v$, iterating until convergence. For ``nice'' matrices, this converges rapidly to an eigenvector; for ``nicer'' ones, the result is always the unique dominant eigenvector. MCMC schemes essentially rely on the same idea: for any initial distribution $f$, and for a Markov kernel $K$, turn it into a sample from $Lf=\int K(\cdot, y)f(y)dy$, and iterate; samples from $L^Pf$ for some large integer $P$ are good enough because $L^Pf$ converges to $\pi$. 

We claim that $\hat{L}^P(f)$ can be computed by replacing ${M}$ in equation (\ref{eqn:khatdef}) with $(MC)^{P-1}M$. For example,

\begin{align*}
(\hat{L}^2f)(x) &= \int \sum_{k,\ell=1}^B M_{k\ell}h_k(x)h_\ell(z)\int \sum_{i,j=1}^B M_{ij} h_i(z)h_j(y)f(y)dydz\\
&=  \sum_{k,\ell, i,j=1}^B M_{k\ell} M_{ij}h_k(x)\int h_\ell(z)  h_i(z)dz\int h_j(y)f(y)dy\\
&=  \sum_{k,\ell, i,j=1}^B M_{k\ell} M_{ij}h_k(x)c_{\ell i}\int h_j(y)f(y)dy\\
&=  \sum_{k,j=1}^B h_k(x)(MCM)_{kj}\int h_j(y)f(y)dy\\
\end{align*}

 \begin{align*}
(\hat{L}^3f)(x) &= \int \sum_{k,\ell=1}^B (MCM)_{k\ell}h_k(x)h_\ell(z)\int \sum_{i,j=1}^B M_{ij} h_i(z)h_j(y)f(y)dydz\\
&=  \sum_{k,\ell, i,j=1}^B (MCM)_{k\ell} M_{ij}h_k(x)\int h_\ell(z)  h_i(z)dz\int h_j(y)f(y)dy\\
&=  \sum_{k,\ell, i,j=1}^B (MCM)_{k\ell} M_{ij}h_k(x)c_{\ell i}\int h_j(y)f(y)dy\\
&=  \sum_{k,j=1}^B h_k(x)(MCMCM)_{kj}\int h_j(y)f(y)dy\\
\end{align*}


\EMK{Issues remain in this paragraph. Not convinced I'm getting the eigenvector of the correct matrix.}
The matrix $(MC)^{P-1}M$ can be rewritten as $C^{-1/2}(C^{1/2}MC^{1/2})^PC^{-1/2}$, which suggests computing a dominant eigenvector $v'$ for $C^{1/2}MC^{1/2}$ and then getting $v\equiv C^{-1/2} v'$.
A natural approximation for the steady state of $K$ is the linear combination of $\{h_i\}_{i=1}^B$ using values of $v$ as coefficients.

\EMK{This may not be the best way to justify this approximation. Need more math!}

\begin{algorithm}[h]
\caption{BEMC algorithm--stage two}
Given an estimate $\hat{G}$ of $G$:\\
For each $i,j$ pair, compute $C_{ij}$ as $(\frac{1}{2\pi (\sigma_i+\sigma_j)^2})^{\frac{D}{2}}\exp(\frac{-(\mu_i-\mu_j)^2}{2\sigma_i^2+2\sigma_j^2})$.\\
Compute $\hat{M}=C^{-1}\hat{G}C^{-1}$.\\
Compute the leading eigenvector $v$ of $M$. \\
Return $\sum_i v_i h_i$ a posterior estimate\\
\end{algorithm}

%\subsection{BEMC-G, a Gibbs sampling variant}
%\label{sec:BEMC-G}
%This approximation can also be adapted to Gibbs sampling, a ubiquitous MCMC variant. 

\subsection{BEMC-R, a variant modeling rejections}
\label{sec:BEMC-R}

As we mention in section \ref{sec:BEMC}, our scheme is able to model continuous kernels. On the other hand, the Metropolis-Hastings algorithm sometimes rejects proposed samples, so its kernel will have a Dirac delta spike, a singularity, wherever the next equals the current value. In this section, we introduce a variant of BEMC that explicitly models rejections by the sampler. 

Let us look at the Metropolis-Hastings kernel in more detail. Going back to the algorithm, the quantity $\alpha(x_{}|x_{i-1})$ is the probability of rejecting a move from $x_{i-1}$ to $x$. For convenience, let $\alpha(x_{i-1})$ denote the (overall) probability of rejecting a move from $x_{i-1}$. Splitting up the next draw as an alternative between moving and staying put, we can write $K(x_2, x_1) = \alpha(x_1)\delta_{x_1}(x_2) + (1-\alpha(x_1))a(x_2|x_1)$. In this expression, $a(x_2|x_1)$ is the conditional density of $x_2$ given that our move out of $x_1$ was not rejected. This is not the same as $q(x_2|x_1)$, since the lack of rejection informs us that we have more likely moved into a region of higher probability. To set up the last line below, define $D_{\alpha}$ from $\alpha$ so that $(D_{\alpha}f)(x)\equiv \alpha(x)f(x)$, and let $(L_{acc}f)(x)\equiv \int a(x|y)(1-\alpha(y))f(y)dy$. Then:
\begin{align*}
 f_2(x_2) &= \int K(x_2, x_1)f_1(x_1)dx_1 \\
&= \int (\alpha(x_1)\delta_{x_1}(x_2) + (1-\alpha(x_1))a(x_2|x_1))f_1(x_1)dx_1 \\
&= \int \alpha(x_1)\delta_{x_2}(x_1)f_1(x_1)dx_1 + \int (1-\alpha(x_1))a(x_2|x_1)f_1(x_1)dx_1 \\
&=  \alpha(x_2)f_1(x_2) + \int (1-\alpha(x_1))a(x_2|x_1)f_1(x_1)dx_1 \\
&=  (D_{\alpha}f_1)(x_2) + (L_{acc}f_1)(x_2) \\
\end{align*}

We can sample from a pdf proportional to $D_{\alpha}f$ by sampling $z$ from $f(x)$, then running an M-H iteration on $z$ to get $w$ and retaining the sample $z$ if $w \neq z$. We can sample from a pdf proportional to $L_{acc}f$ by doing nearly the same steps, but retaining $w$ if $w \neq z$. These facts will be useful as we attempt to estimate $L_{acc}$.

This time around, we will try to estimate a function $\hat{\alpha}$ and a matrix $M$ so that $\hat{\alpha}\approx{\alpha}$ and $L_{acc} \approx \hat{L}_{acc}$, where $(\hat{L}_{acc}f)(x) =\sum_{i,j=1}^B h_i(x)M_{ij}\int h_j(x)f(x)dx$. Even if the parameters were chosen optimally, $L$ may not take the same form as $\hat{L}_{\alpha}+\hat{L}_{M}$, so the estimate $\hat{\pi}$ will not be correct. \EMK{Need some results answering ``in what sense is your method (approximately) correct?''}

For this variant, we need still need to estimate $M$, but with the added complication of trying to infer $\hat{\alpha}$ at the same time. Fortunately, it is easy to tell when the sampler rejects and when it doesn't, and this provides a way to tease out information about $\alpha$. Suppose for a moment that we start the sampler at a point $z$ and it takes a single step to $w$. If $w \neq z$, then the sampler has shown less of a tendency to reject starting from $z$, and we label $z$ with a $0$. If $w = z$, we label $z$ with a $1$. Once the sample space is covered in zeroes and ones, there are many probabilistic classifier methods that could give an estimate of $\hat{\alpha}$, which at any given point is just the probability of labeling with a one. Meanwhile, whenever the sampler moves, we gain information about $L_{acc}$, and we can update $M$ as before. 

This strategy still throws away useful information. To see why, recall that the Metropolis-Hastings algorithm makes a proposal, computes an rejection probability, flips a proverbial coin with that probability, and then discards the rejection probability. When drawing a chain of samples, the rejection probability serves no further purpose, so discarding it is natural. In BEMC-R, though, it provides a more efficient estimate of $\alpha$. If the rejection probability when proposing a move to $w$ from $z$ is $p$, then the better procedure is to label $z$ with $p$. Likewise, instead of updating the estimate of $M_{ij}$ with sample of weight $1$ with probability $p$, we can update it with a sample of weight $p$.

\begin{algorithm}[h]
\caption{BEMC-R algorithm--stage one}
Set $M$ to $0$.\\ 
Set a scalar $W$ to zero. $W$ is the effective number of samples in an estimate of an entry of $M$.\\
Set $T = \{\}$. $T$ will be the training set for $\hat{\alpha}$.\\
For $b_{in}  = 1:B$\\
\Indp
For $b_{out}  = 1:B$\\
\Indp
For $n = 1:N$\\
\Indp
Draw a sample $z_n$ from $h_{b_{in}}$.\\
Draw a proposal $w_n$ and compute its rejection probability $p$.\\
Add $(z_n, p)$ to $T$.\\
Increment $M_{b_{out}, b_{in}}$ by $ph_{b_{in}(w_n)}$.\\
Increment $W$ by $p$.\\
\Indm
Divide $M_{b_{out}, b_{in}}$ by $W$.\\
\Indm
\Indm
Train $\hat{\alpha}$ on $T$.\\
\end{algorithm}

For one further refinement, we could include some prior information about $M$. Since $L_{acc}$ mimics the action of the sampler as it moves, it should resemble the action of the proposal alone, with no rejections. That would mean $M_{ij} \approx \int h_i(x)q(x|y)h_j(y)dydx$. For simple proposal distributions like a uniform or normal centered on the current value, this integral may be easy to find as a convolution.

\subsection{Computing the steady state in BEMC-R}
 Given $\hat{M}$, $\hat{\alpha}$, and an initial state $f_0$, we want to compute $[D_{\hat{\alpha}}+\hat{L}_{acc}]^P(f_0)$ for some moderately high exponent $P$. To simplify the problem, suppose we set $f_0$ to $h_1$, one of the initial $B$ basis functions. Also, suppose that we restrict $\hat{\alpha}$ to a form where for any of our basis functions $h_i$, we can expand $\hat{\alpha}h_i$ as a sum $\sum_{i=1}^B d_i h_i$. 
Computing $\hat{L}_{acc}(f_0)$ is simple: $\hat{L}_{acc}(f_0) = \sum_{i=1}^B \hat{L}_{acc}(d_i h_i) = \sum_{i=1}^B [\hat{M}c]_i h_i$ \EMK{Missing factor of C somewhere}. The difficulty lies in finding a representation of $D_{\hat{\alpha}}(f_0)$ in this basis, i.e. evaluating or quickly approximating integrals of the form $\int_{\Omega} h_i(x)h_j(x)\hat{\alpha}(x)dx$. \EMK{Maybe we'll choose a crafty form for $\hat{\alpha}$ and do this analytically. }


%==== Bib files and style =======
\bibliographystyle{splncs}
\bibliography{eric_kernfeld_biblio}

\end{document}
